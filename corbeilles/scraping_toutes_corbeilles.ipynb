{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's start with democratie and citoyennete\n",
    "# this will make us log in\n",
    "# going to our page\n",
    "driver.get('https://granddebat.fr/project/democratie-et-citoyennete-1/collect/participez-a-la-recherche-collective-de-solutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging in\n",
    "button = driver.find_element_by_id('trash-link')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", button)\n",
    "button.click()\n",
    "driver.find_element_by_class_name('btn-success').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.find_element_by_id('username').send_keys('v.viers@lse.ac.uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.find_element_by_id('password').send_keys('elGranDebato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.find_element_by_id('confirm-login').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on right link after logging in\n",
    "button = driver.find_element_by_id('trash-link')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", button)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all links to scrape and info aboute each link\n",
    "# note that this script is not adapted to multiple page yet\n",
    "# but it would just take an additional while loop \n",
    "# in which you'd make selenium click on the next button everytime\n",
    "proposition_container = driver.find_element_by_class_name('proposal-preview-trash_list')\n",
    "propositions = proposition_container.find_elements_by_class_name('block')\n",
    "\n",
    "all_links = []\n",
    "for proposition in propositions:\n",
    "    new_link = {}\n",
    "    header = proposition.find_element_by_class_name('proposal__title')\n",
    "    new_link['title'] = header.find_element_by_tag_name('a').text\n",
    "    new_link['link'] = header.find_element_by_tag_name('a').get_attribute('href')\n",
    "    new_link['reason'] = proposition.find_element_by_class_name('proposal__trashed-reason').text\n",
    "    new_link['author'] = proposition.find_element_by_class_name('proposal__author').text\n",
    "    new_link['date'] = proposition.find_element_by_class_name('proposal__date').text\n",
    "    # storing in list \n",
    "    all_links.append(new_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turning list into dataframe\n",
    "df_to_scrape = pd.DataFrame(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to scrape content of each page\n",
    "def scraping_each_proposition(row): \n",
    "    driver.get(row['link'])\n",
    "    main = driver.find_element_by_class_name('main')\n",
    "    driver.implicitly_wait(3) # wait for page to set up\n",
    "    question_blocks = main.find_elements_by_class_name('block')\n",
    "    question_scraped = {}\n",
    "    for question_block in question_blocks:\n",
    "        try:\n",
    "            text_question = question_block.find_element_by_tag_name('h3').text\n",
    "            try:\n",
    "                text_answer = question_block.find_element_by_tag_name('p').text\n",
    "            except:    \n",
    "                text_answer = 'NONE'\n",
    "            question_scraped['\"' + text_question + '\"'] = text_answer\n",
    "        except:\n",
    "            continue\n",
    "    question_scraped['link'] = row['link']\n",
    "    # returning pd.Series object\n",
    "    # it will have the link\n",
    "    # and then each question will be column\n",
    "    # and each answer will be the content of a cell\n",
    "    # each proposition will be a row in our dataframe, when calling the function\n",
    "    return pd.Series(question_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calling function\n",
    "here = df_to_scrape.apply(scraping_each_proposition, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merging the dataframe with links to scrape and scraped pages\n",
    "merged_df = here.merge(df_to_scrape, right_on = 'link', left_on = 'link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving as csv\n",
    "merged_df.to_csv('democratie_et_citoyennete_corbeille.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now taking it further by scraping all corbeilles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating list of all remaining corbeilles\n",
    "corbeilles = ['https://granddebat.fr/projects/lorganisation-de-letat-et-des-services-publics/trashed', 'https://granddebat.fr/projects/la-transition-ecologique/trashed', 'https://granddebat.fr/projects/la-fiscalite-et-les-depenses-publiques/trashed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corbeille in corbeilles:\n",
    "    # we are already logged in\n",
    "    driver.get(corbeille)\n",
    "    button = driver.find_element_by_id('trash-link')\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true)\", button)\n",
    "    button.click()\n",
    "    proposition_container = driver.find_element_by_class_name('proposal-preview-trash_list')\n",
    "    propositions = proposition_container.find_elements_by_class_name('block')\n",
    "    # doing as previously\n",
    "    all_links = []\n",
    "    for proposition in propositions:\n",
    "        new_link = {}\n",
    "        header = proposition.find_element_by_class_name('proposal__title')\n",
    "        new_link['title'] = header.find_element_by_tag_name('a').text\n",
    "        new_link['link'] = header.find_element_by_tag_name('a').get_attribute('href')\n",
    "        new_link['reason'] = proposition.find_element_by_class_name('proposal__trashed-reason').text\n",
    "        new_link['author'] = proposition.find_element_by_class_name('proposal__author').text\n",
    "        new_link['date'] = proposition.find_element_by_class_name('proposal__date').text\n",
    "        # storing in list \n",
    "        all_links.append(new_link)\n",
    "    # our df of links to scrape\n",
    "    df_to_scrape = pd.DataFrame(all_links)\n",
    "    # actually scraping\n",
    "    here = df_to_scrape.apply(scraping_each_proposition, axis=1)\n",
    "    # merging\n",
    "    merged_df = here.merge(df_to_scrape, right_on = 'link', left_on = 'link')\n",
    "    # getting the right slug for our csv\n",
    "    reg = re.compile(r'projects/(.*)/trashed')\n",
    "    title_csv = str(reg.search(corbeille).group(1)).replace(\"'\", \"\")\n",
    "    # exporting\n",
    "    merged_df.to_csv( title_csv + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
